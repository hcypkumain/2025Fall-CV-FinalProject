\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
 \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{textcomp}       % additional text symbols and fonts
\usepackage{lmodern}        % Latin Modern font (enhanced Computer Modern)
\usepackage{adjustbox}      % box and alignment utilities
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\definecolor{lightred}{rgb}{0.95, 0.8, 0.8}
\usepackage{graphicx}

\usepackage{amsmath, amssymb, bm, amsfonts}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{subcaption}
\captionsetup[subfigure]{labelfont=bf, font=small} % 子图标签加粗、字号稍小（符合NeurIPS风格）
\usepackage{array}
\usepackage{booktabs}
\usepackage{makecell}


\usepackage[ruled,linesnumbered]{algorithm2e}
\title{Heavy Machine Out, We Have 3DGS Now!}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
%   % examples of more authors
%   \And
  Chunyu He \\
  Department of Computer Science \\
  Peking University \\
  Beijing, China 100871 \\
  \texttt{chunyuhe25@stu.pku.edu.cn} \\
\And
  Zijie Xu \\
  Department of Computer Science \\
  Peking University \\
  Beijing, China 100871 \\
  \texttt{zjxu25@stu.pku.edu.cn} \\
\And
  Peng Ma \\
  Department of Computer Science \\
  Peking University \\
  Beijing, China 100871 \\
  \texttt{pma25@stu.pku.edu.cn} \\
\And
  Bingrui Guo \\
  Department of Computer Science \\
  Peking University \\
  Beijing, China 100871 \\
  \texttt{bguo9894@stu.pku.edu.cn} \\
\And
  Ruiqi Li \\
  Department of Computer Science \\
  Peking University \\
  Beijing, China 100871 \\
  \texttt{rqli25@stu.pku.edu.cn} \\
}


\begin{document}


\maketitle


\begin{abstract}
High-fidelity 3D facial reconstruction is crucial for clinical and consumer applications, yet existing professional systems are prohibitively expensive, immobile, and require controlled environments and expert operation. While recent advances in neural rendering offer promise, reconstructing clinically viable, watertight meshes from casually captured monocular smartphone video remains challenging due to motion blur, limited viewpoints, and illumination variability. We propose PFV-3D, a practical end-to-end framework that leverages the expressive power of 3D Gaussian Splatting (3DGS) to democratize high-quality 3D face modeling. Our pipeline integrates fisheye-aware camera modeling, robust pose initialization, and topology-aware mesh extraction to directly generate detailed, watertight 3D face meshes from short, handheld videos captured by everyday smartphones. We demonstrate that PFV-3D achieves reconstruction fidelity comparable to professional 3dMD systems at a fraction of the cost and complexity, enabling scalable and accessible 3D facial modeling without sacrificing clinical utility.The code and models for PFV-3D are available at \url{https://github.com/hcypkumain/2025Fall-CV-FinalProject.git}.
\end{abstract}

\section{Introduction}
High-fidelity 3D facial reconstruction has long been a cornerstone in applications ranging from clinical diagnostics and plastic surgery planning to virtual avatars and biometric authentication. Traditionally, this task has relied on specialized multi-view or structured-light systems---such as the commercial 3dMD suite---which, while capable of sub-millimeter accuracy, impose significant barriers in terms of cost, portability, and operational complexity. These systems typically require controlled lighting, calibrated multi-camera rigs, and expert supervision, rendering them impractical for point-of-care settings, telemedicine, or large-scale deployment in resource-constrained environments.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{compare.pdf}
  \caption{Comparison between our phone-based method and existing heavy machine-based approaches.}
  \label{fig:compare}
\end{figure}

Recent advances in neural rendering and geometry representation---particularly 3D Gaussian Splatting (3DGS) and its downstream meshing pipelines like GS2Mesh---have opened new avenues for high-quality 3D reconstruction from unstructured, casually captured video. Unlike traditional methods, these approaches can leverage monocular input from commodity mobile devices, democratizing access to 3D facial modeling without sacrificing geometric fidelity. However, bridging the gap between unconstrained smartphone footage and clinically viable reconstructions remains challenging due to issues such as motion blur, limited viewpoint coverage, and illumination variability.

In this work, we propose a practical, end-to-end framework that harnesses the expressive power of Gaussian-based representations to reconstruct detailed, watertight 3D face meshes directly from short, handheld videos captured by everyday smartphones. By integrating robust pose initialization, adaptive densification, and topology-aware mesh extraction, our pipeline not only bypasses the need for expensive hardware but also simplifies the user workflow to a ``point-and-shoot'' experience. We demonstrate that our method achieves reconstruction quality comparable to professional 3dMD systems at a fraction of the cost and complexity, thereby enabling scalable, accessible 3D facial modeling for both medical and consumer applications.

\section{Related Work}

\paragraph{Classical and Model-Based 3D Face Reconstruction.}
Early approaches to 3D face reconstruction relied heavily on multi-view stereo~\cite{seitz1999phototourism}, structured light~\cite{zhang2010recent}, or laser scanning---technologies that underpin commercial systems like 3dMD. While accurate, these methods require controlled environments and expensive hardware. To enable reconstruction from a single image, the 3D Morphable Model (3DMM)~\cite{blanz1999morphable} was introduced, representing faces as linear combinations of shape and texture bases derived from statistical analysis of 3D scans. Subsequent works extended 3DMM with nonlinear deformations~\cite{cao2014facewarehouse} or combined it with photometric stereo~\cite{aldrian2013inverse}. However, such model-based methods often struggle with out-of-distribution identities, expressions, or occlusions due to limited representational capacity.

\paragraph{Learning-Based Monocular 3D Face Reconstruction.}
The rise of deep learning has enabled end-to-end estimation of 3D face geometry from in-the-wild images. Early CNN-based methods regressed 3DMM parameters directly~\cite{tran2017regressing, richardson2017learning}, while later works leveraged self-supervision by enforcing consistency between input images and differentiable renderings~\cite{tewari2017mofa, genova2018unsupervised}. Notably, RingNet~\cite{ringnet2019} and DECA~\cite{feng2021learning} achieved impressive results using only 2D landmark or identity supervision, eliminating the need for ground-truth 3D data. More recently, implicit representations such as Signed Distance Functions (SDFs)~\cite{zheng2022pifuhd} and neural radiance fields~\cite{wang2021neural} have been adapted to human faces, enabling high-resolution geometry and view-consistent appearance synthesis. Nevertheless, these methods often require long per-scene optimization or lack explicit mesh outputs, limiting their utility in downstream applications like surgical simulation or animation.

\paragraph{Neural Rendering and Gaussian Splatting.}
Neural Radiance Fields (NeRF)~\cite{mildenhall2020nerf} revolutionized novel view synthesis by modeling scenes as continuous volumetric functions. Extensions like Instant-NGP~\cite{mueller2022instant} and Scaffold-GS~\cite{felzenszwalb2023scaffold} dramatically accelerated training and rendering, making real-time applications feasible. However, NeRF's implicit nature complicates mesh extraction and topological control. In contrast, 3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} represents scenes as collections of anisotropic Gaussians, enabling real-time, high-fidelity rendering without neural networks at test time. Recent works have applied 3DGS to human avatars~\cite{chen2024gaussianavatar} and dynamic faces~\cite{liu2024headgs}, demonstrating its potential for expressive and efficient reconstruction. Crucially, pipelines like GS2Mesh~\cite{yu2024gs2mesh} bridge the gap between splatting and explicit geometry by converting Gaussians into watertight meshes via Poisson surface reconstruction or learned deformation fields---making 3DGS viable for clinical and industrial use cases.

\paragraph{Accessible 3D Capture for Medical Applications.}
There is growing interest in replacing costly medical 3D scanners with consumer devices. Prior efforts include using stereo cameras~\cite{shao2021low}, depth sensors (e.g., Kinect)~\cite{salazar2014accuracy}, or photogrammetry from smartphones~\cite{gander2020smartphone}. However, these often suffer from noise, holes, or poor texture fidelity. Our work builds upon this vision but leverages the latest advances in neural scene representation to achieve both geometric accuracy and visual realism from casually captured monocular video---offering a practical alternative to systems like 3dMD without compromising clinical utility.


\section{Methodology}

We propose \textbf{PFV-3D}, a practical and accessible 3D facial reconstruction pipeline that enables high-quality geometry recovery using only a commodity smartphone under typical indoor lighting. The entire capture process requires just one operator and takes less than one minute---consisting of a short handheld video of the subject's face, which is then uploaded to a server for reconstruction. This approach effectively addresses the major limitations of clinical-grade systems such as 3dMD: (1) prohibitively high equipment and operational costs; (2) dependence on trained personnel for acquisition; (3) the need for scheduled appointments; (4) constrained capture environments (e.g., controlled lighting and fixed multi-camera rigs); and (5) a non-negligible failure rate---estimated at approximately one-third in routine clinical use due to motion artifacts, poor cooperation, or suboptimal positioning. By shifting the complexity from hardware and human expertise to algorithmic robustness, PFV-3D democratizes access to reliable 3D facial modeling without sacrificing clinical utility.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{framework_image.pdf}
  \caption{Overview of the PFV-3D framework.}
  \label{fig:framework}
\end{figure}

Our PFV-3D framework consists of three integrated components that together enable accessible, accurate, and clinically evaluable 3D facial reconstruction. First, data acquisition is performed using an off-the-shelf smartphone camera under uncontrolled indoor lighting; the user captures a short ($\approx60$-second) handheld video of the subject's face with minimal instruction, eliminating the need for specialized hardware or trained operators. Second, we introduce a tailored reconstruction pipeline built upon GS2Mesh~\cite{yu2024gs2mesh}, which we adapt to handle monocular, casually captured facial sequences. This stage first reconstructs a radiance field in the form of 3D Gaussians~\cite{kerbl20233d} optimized from the input video, then converts the resulting splatting representation into a watertight, high-fidelity mesh suitable for geometric analysis. Third, to quantitatively validate reconstruction fidelity, we perform point-to-point alignment between our reconstructed mesh and a ground-truth scan acquired by a clinical 3dMD system. Using robust point cloud registration (e.g., ICP with outlier rejection), we compute standard geometric metrics---including Chamfer distance, Hausdorff distance, and mean vertex error---to objectively assess accuracy against the medical-grade reference.




% 定义常用符号
\newcommand{\R}{\mathbb{R}}
\newcommand{\SO}{\mathrm{SO}(3)}
\newcommand{\T}{\top}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert_2}
\newcommand{\proj}{\pi_{\mathrm{KB}}}
\newcommand{\atan}{\operatorname{atan2}}

\subsection{Fisheye-Aware Camera Model for Multi-View Reconstruction}
\label{subsec:fisheye_model}



To enable robust 3D reconstruction from smartphone fisheye cameras (e.g., ultra-wide lenses with FoV $\geq 120^\circ$), we replace the standard pinhole projection in GS2MeshPipeline with the \emph{Kannala--Brandt (KB)} fisheye model~\cite{kannala2006generic}, which accurately captures radial distortion up to $180^\circ$. Let $\mathbf{X} \in \R^3$ denote a 3D point in the world frame. Its coordinates in the $i$-th camera frame are given by the rigid transformation:
\begin{equation}
    \mathbf{X}_c^{(i)} = \mathbf{R}^{(i)} \mathbf{X} + \mathbf{t}^{(i)}, \quad 
    \mathbf{R}^{(i)} \in \SO,\; \mathbf{t}^{(i)} \in \R^3.
    \label{eq:rigid_transform}
\end{equation}
Define the unit direction vector $\mathbf{u}^{(i)} = \mathbf{X}_c^{(i)} / \norm{\mathbf{X}_c^{(i)}}$, and let $\theta^{(i)} = \arccos(u_z^{(i)}) \in [0, \pi]$ be the incident angle w.r.t. the optical axis.

The KB projection maps $(\theta^{(i)}, \phi^{(i)})$ to pixel coordinates via:
\begin{align}
    r^{(i)} &= f(\theta^{(i)}) 
    = \theta^{(i)} + k_1 (\theta^{(i)})^3 + k_2 (\theta^{(i)})^5 
      + k_3 (\theta^{(i)})^7 + k_4 (\theta^{(i)})^9,
    \label{eq:kb_radial} \\
    \phi^{(i)} &= \atan(u_y^{(i)}, u_x^{(i)}), \\
    x^{(i)} &= c_x^{(i)} + f_x^{(i)} \, r^{(i)} \cos\phi^{(i)}, \\
    y^{(i)} &= c_y^{(i)} + f_y^{(i)} \, r^{(i)} \sin\phi^{(i)},
    \label{eq:kb_pixel}
\end{align}
where $\mathbf{K}^{(i)} = \mathrm{diag}(f_x^{(i)}, f_y^{(i)}, 1)$ is the intrinsic matrix, $(c_x^{(i)}, c_y^{(i)})$ the principal point, and $\mathbf{k}^{(i)} = [k_1, k_2, k_3, k_4]^\T$ the radial distortion coefficients.

The full intrinsic parameter vector is thus:
\begin{equation}
    \boldsymbol{\theta}_{\mathrm{int}}^{(i)} = \big[ f_x^{(i)},\, f_y^{(i)},\, c_x^{(i)},\, c_y^{(i)},\, k_1^{(i)},\, k_2^{(i)},\, k_3^{(i)},\, k_4^{(i)} \big]^\T \in \R^8.
    \label{eq:int_params}
\end{equation}
For extrinsics, we adopt the minimal 6-DoF representation using the rotation vector $\boldsymbol{\omega}^{(i)} \in \R^3$ (via exponential map) and translation $\mathbf{t}^{(i)} \in \R^3$:
\begin{equation}
    \boldsymbol{\theta}_{\mathrm{ext}}^{(i)} = \big[ (\boldsymbol{\omega}^{(i)})^\T,\, (\mathbf{t}^{(i)})^\T \big]^\T \in \R^6,
    \quad \mathbf{R}^{(i)} = \exp\big([\boldsymbol{\omega}^{(i)}]_\times\big),
    \label{eq:ext_params}
\end{equation}
where $[\cdot]_\times$ denotes the skew-symmetric operator.

Given $N$ views and $M_i$ correspondences $\{(\mathbf{x}_{ij}, \mathbf{X}_j)\}_{j=1}^{M_i}$ per view, the joint optimization for structure-from-motion (SfM) or bundle adjustment (BA) becomes:
\begin{equation}
    \min_{\substack{
        \{\boldsymbol{\theta}_{\mathrm{int}}^{(i)}\},\, \{\boldsymbol{\theta}_{\mathrm{ext}}^{(i)}\},\\
        \{\mathbf{X}_j\}
    }}
    \sum_{i=1}^N \sum_{j=1}^{M_i}
    \Big\| 
        \proj\!\big(\mathbf{R}^{(i)} \mathbf{X}_j + \mathbf{t}^{(i)};\, \boldsymbol{\theta}_{\mathrm{int}}^{(i)}\big)
        - \mathbf{x}_{ij}
    \Big\|_2^2,
    \label{eq:ba_loss}
\end{equation}
where $\proj(\cdot)$ implements the forward KB projection (Eqs.~\ref{eq:kb_radial}--\ref{eq:kb_pixel}).

Crucially, for gradient-based optimization, we require the \emph{inverse projection} (back-projection) to compute Jacobians. Given pixel $(x,y)$, we first normalize:
\begin{equation}
    \tilde{x} = \frac{x - c_x}{f_x}, \quad 
    \tilde{y} = \frac{y - c_y}{f_y}, \quad 
    r = \sqrt{\tilde{x}^2 + \tilde{y}^2}, \quad 
    \phi = \atan(\tilde{y}, \tilde{x}).
    \label{eq:normalization}
\end{equation}
Then solve $r = f(\theta)$ for $\theta$ via Newton–Raphson iteration:
\begin{equation}
    \theta_{n+1} = \theta_n - \frac{f(\theta_n) - r}{f'(\theta_n)}, \quad
    f'(\theta) = 1 + 3k_1\theta^2 + 5k_2\theta^4 + 7k_3\theta^6 + 9k_4\theta^8,
    \label{eq:newton}
\end{equation}
initialized at $\theta_0 = r$. The back-projected ray direction is:
\begin{equation}
    \mathbf{u} = 
    \begin{bmatrix}
        \sin\theta \cos\phi \\
        \sin\theta \sin\phi \\
        \cos\theta
    \end{bmatrix}.
    \label{eq:backproj_ray}
\end{equation}
This enables exact analytic Jacobians of the reprojection error w.r.t. both intrinsic and extrinsic parameters (see Appendix~\ref{app:jacobian} for derivation).

Our pipeline integrates this model into GS2Mesh by replacing the pinhole \texttt{Camera} class with a \texttt{FisheyeCamera} that implements Eqs.~\eqref{eq:kb_radial}--\eqref{eq:kb_pixel} and supports automatic differentiation (e.g., via PyTorch).

\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\SetKwProg{Fn}{Function}{}{}
\SetKw{KwTo}{to}
\SetKwFor{For}{for}{do}{endfor}
\SetKwIF{If}{ElseIf}{Else}{if}{then}{else if}{else}{endif}

% --- 修正：禁用 \Break，改用显式 break 文本 ---
\begin{algorithm}[t]
\caption{KB-BA: Fisheye-Aware Bundle Adjustment}
\label{alg:kb_ba}
\DontPrintSemicolon
\KwInput{
    $\{(\mathbf{x}_{ij}, \mathbf{X}_j)\}_{i=1..N,\,j=1..M_i}$: 2D--3D correspondences; \\
    Initial intrinsics $\boldsymbol{\theta}_{\mathrm{int}}^{(i)}$, extrinsics $\boldsymbol{\theta}_{\mathrm{ext}}^{(i)}$, and 3D points $\{\mathbf{X}_j\}$.
}
\KwOutput{Optimized parameters $\{\boldsymbol{\theta}_{\mathrm{int}}^{(i)*}\}, \{\boldsymbol{\theta}_{\mathrm{ext}}^{(i)*}\}, \{\mathbf{X}_j^*\}$.}

\For{$\text{iter} = 1$ \KwTo $\text{max\_iters}$}{
    $\mathbf{J} \gets \mathbf{0},\; \mathbf{r} \gets \mathbf{0}$ \tcp*[r]{Initialize Jacobian \& residual}
    
    \For{$i = 1$ \KwTo $N$}{
        \For{$j = 1$ \KwTo $M_i$}{
            $\mathbf{X}_c \gets \mathbf{R}^{(i)} \mathbf{X}_j + \mathbf{t}^{(i)}$ \;
            $\mathbf{u} \gets \mathbf{X}_c / \norm{\mathbf{X}_c}$ \;
            $\theta \gets \arccos(\max(-1, \min(1, u_z)))$ \;
            $r_{\text{pred}} \gets \theta + \sum_{\ell=1}^4 k_\ell \theta^{2\ell+1}$ \;
            $\phi \gets \atan(u_y, u_x)$ \;
            $x_{\text{pred}} \gets c_x + f_x r_{\text{pred}} \cos\phi$ \;
            $y_{\text{pred}} \gets c_y + f_y r_{\text{pred}} \sin\phi$ \;
            
            $\mathbf{r}_{ij} \gets 
            \begin{bmatrix}
                x_{\text{pred}} - x_{ij} \\ y_{\text{pred}} - y_{ij}
            \end{bmatrix}$ \;
            
            % Jacobians (computed analytically or via autodiff)
            $\mathbf{J}_{ij}^{\mathrm{int}} \gets \partial \mathbf{r}_{ij} / \partial \boldsymbol{\theta}_{\mathrm{int}}^{(i)}$ \;
            $\mathbf{J}_{ij}^{\mathrm{ext}} \gets \partial \mathbf{r}_{ij} / \partial \boldsymbol{\theta}_{\mathrm{ext}}^{(i)}$ \;
            $\mathbf{J}_{ij}^{\mathrm{X}} \gets \partial \mathbf{r}_{ij} / \partial \mathbf{X}_j$ \;
            
            Accumulate residuals and Jacobian blocks\;
        }
    }
    
    $\Delta \boldsymbol{\theta} \gets 
    \left( \mathbf{J}^\T \mathbf{J} + \lambda \mathbf{I} \right)^{-1} \mathbf{J}^\T \mathbf{r}$ \;
    Update: $\boldsymbol{\theta} \gets \boldsymbol{\theta} - \Delta \boldsymbol{\theta}$ \;
    
    \If{$\|\Delta \boldsymbol{\theta}\| < \epsilon$}{
        \textbf{break}\;  % ✅ 安全替代 \Break
    }
}
\Return $\boldsymbol{\theta}^*$\;
\end{algorithm}

\subsection{Mathematical Formulation of Rigid Registration via Corresponding Landmarks}
\label{subsec:math_registration}

Given two sets of $N$ corresponding 3D facial landmarks:
\begin{itemize}
    \item Target point set: $\mathcal{P} = \{\mathbf{p}_i \in \mathbb{R}^3\}_{i=1}^{N}$ (e.g., from GS2Mesh),
    \item Reference point set: $\mathcal{Q} = \{\mathbf{q}_i \in \mathbb{R}^3\}_{i=1}^{N}$ (e.g., from 3DMD),
\end{itemize}
the goal is to estimate the optimal rigid transformation—comprising a rotation matrix $\mathbf{R} \in \mathrm{SO}(3)$ and a translation vector $\mathbf{t} \in \mathbb{R}^3$—that minimizes the sum of squared Euclidean distances between transformed target points and reference points:

\begin{equation}
\min_{\mathbf{R},\,\mathbf{t}} \; J(\mathbf{R}, \mathbf{t}) 
= \sum_{i=1}^{N} \left\| \mathbf{R}\,\mathbf{p}_i + \mathbf{t} - \mathbf{q}_i \right\|_2^2,
\quad \text{subject to } \mathbf{R}^\top \mathbf{R} = \mathbf{I},\; \det(\mathbf{R}) = +1.
\label{eq:kabsch_objective}
\end{equation}

The solution proceeds in three analytical steps:

\paragraph{Step 1: Centroid removal (translation decoupling)}  
Compute centroids of both point sets:
\begin{align}
\bar{\mathbf{p}} &= \frac{1}{N}\sum_{i=1}^{N} \mathbf{p}_i, &
\bar{\mathbf{q}} &= \frac{1}{N}\sum_{i=1}^{N} \mathbf{q}_i.
\end{align}
Define centered coordinates:
\begin{align}
\tilde{\mathbf{p}}_i &= \mathbf{p}_i - \bar{\mathbf{p}}, &
\tilde{\mathbf{q}}_i &= \mathbf{q}_i - \bar{\mathbf{q}}.
\end{align}
Substituting $\mathbf{t} = \bar{\mathbf{q}} - \mathbf{R}\bar{\mathbf{p}}$ into Eq.~\eqref{eq:kabsch_objective} eliminates the translation term, reducing the problem to pure rotation estimation:
\begin{equation}
\min_{\mathbf{R} \in \mathrm{SO}(3)} \; \sum_{i=1}^{N} \left\| \mathbf{R}\,\tilde{\mathbf{p}}_i - \tilde{\mathbf{q}}_i \right\|_2^2.
\label{eq:kabsch_rotation}
\end{equation}

\paragraph{Step 2: Optimal rotation via SVD (Kabsch algorithm)}  
Construct the $3 \times 3$ cross-covariance matrix:
\begin{equation}
\mathbf{H} = \sum_{i=1}^{N} \tilde{\mathbf{q}}_i \, \tilde{\mathbf{p}}_i^\top 
= \mathbf{Q} \, \mathbf{P}^\top,
\label{eq:cross_covariance}
\end{equation}
where $\mathbf{P} = [\tilde{\mathbf{p}}_1, \dots, \tilde{\mathbf{p}}_N] \in \mathbb{R}^{3\times N}$ and $\mathbf{Q} = [\tilde{\mathbf{q}}_1, \dots, \tilde{\mathbf{q}}_N] \in \mathbb{R}^{3\times N}$.

Perform singular value decomposition (SVD) of $\mathbf{H}$:
\begin{equation}
\mathbf{H} = \mathbf{U} \, \boldsymbol{\Sigma} \, \mathbf{V}^\top,
\label{eq:kabsch_svd}
\end{equation}
with $\mathbf{U}, \mathbf{V} \in \mathrm{SO}(3)$ (orthogonal matrices) and $\boldsymbol{\Sigma} = \mathrm{diag}(\sigma_1, \sigma_2, \sigma_3)$.

The optimal rotation minimizing Eq.~\eqref{eq:kabsch_rotation} is then given by:
\begin{equation}
\mathbf{R}^\star = \mathbf{U} \, \mathbf{S} \, \mathbf{V}^\top,
\label{eq:kabsch_rotation_matrix}
\end{equation}
where $\mathbf{S} = \mathrm{diag}(1, 1, \det(\mathbf{U}\mathbf{V}^\top))$ ensures $\det(\mathbf{R}^\star) = +1$ (enforcing proper rotation, not reflection).

\paragraph{Step 3: Translation recovery}  
With $\mathbf{R}^\star$ known, the optimal translation follows directly from centroid alignment:
\begin{equation}
\mathbf{t}^\star = \bar{\mathbf{q}} - \mathbf{R}^\star \bar{\mathbf{p}}.
\label{eq:kabsch_translation}
\end{equation}

\paragraph{Post-registration error metric}  
After applying $(\mathbf{R}^\star, \mathbf{t}^\star)$ to $\mathcal{P}$, the residual error for each correspondence is:
\begin{equation}
d_i = \left\| \mathbf{R}^\star \mathbf{p}_i + \mathbf{t}^\star - \mathbf{q}_i \right\|_2,
\quad i = 1,\dots,N,
\label{eq:registration_residual}
\end{equation}
and the root-mean-square error (RMSE) is computed as:
\begin{equation}
\mathrm{RMSE} = \sqrt{ \frac{1}{N} \sum_{i=1}^{N} d_i^2 }.
\label{eq:registration_rmse}
\end{equation}

This closed-form solution is globally optimal under the rigid-body assumption and forms the mathematical backbone of the manual alignment routine implemented in CloudCompare’s \texttt{Align} plugin using user-selected landmark pairs.

\subsection{Point Cloud Similarity Metrics: F1-Score and Chamfer Distance}
\label{subsec:similarity_metrics}

To quantitatively evaluate the geometric fidelity between two facial point clouds—e.g., the reconstructed mesh from GS2Mesh (target) and the ground-truth scan from 3DMD (reference)—we adopt two complementary metrics: the \textbf{F1-score} (a symmetric precision-recall measure) and the \textbf{Chamfer distance} (an asymmetric but differentiable approximation of Hausdorff distance). Both are widely used in 3D shape comparison due to their robustness to sampling density, noise, and partial correspondence.

\paragraph{Chamfer Distance (CD)}  
Let $\mathcal{X} = \{\mathbf{x}_i\}_{i=1}^{N_X} \subset \mathbb{R}^3$ and $\mathcal{Y} = \{\mathbf{y}_j\}_{j=1}^{N_Y} \subset \mathbb{R}^3$ denote the two point sets (e.g., sampled vertices or downsampled points from the meshes). The (symmetric) Chamfer distance is defined as:

\begin{equation}
\mathrm{CD}(\mathcal{X}, \mathcal{Y}) 
= \frac{1}{N_X} \sum_{i=1}^{N_X} \min_{j} \|\mathbf{x}_i - \mathbf{y}_j\|_2^2 
\;+\; 
\frac{1}{N_Y} \sum_{j=1}^{N_Y} \min_{i} \|\mathbf{y}_j - \mathbf{x}_i\|_2^2.
\label{eq:chamfer_distance}
\end{equation}

In practice, the squared Euclidean norm is often used for numerical stability and differentiability (e.g., in deep learning pipelines); the unsquared version (using $\|\cdot\|_2$) is also common in evaluation benchmarks. CD penalizes both *missed structures* (points in $\mathcal{X}$ far from $\mathcal{Y}$) and *hallucinated structures* (points in $\mathcal{Y}$ far from $\mathcal{X}$), making it sensitive to global shape deviation while being computationally efficient ($\mathcal{O}(N_X N_Y)$, or $\mathcal{O}(N \log N)$ with k-d trees).

\paragraph{F1-Score}  
The F1-score is derived from precision and recall computed over nearest-neighbor correspondences at a fixed tolerance threshold $\tau > 0$. Define the set of true positives (TP) as:
\begin{equation}
\mathrm{TP} = \left\{ i \in [1,N_X] \;\middle|\; \min_j \|\mathbf{x}_i - \mathbf{y}_j\|_2 \leq \tau \right\},
\end{equation}
and similarly for reference points:
\begin{equation}
\mathrm{TP}' = \left\{ j \in [1,N_Y] \;\middle|\; \min_i \|\mathbf{y}_j - \mathbf{x}_i\|_2 \leq \tau \right\}.
\end{equation}
Then:
\begin{align}
\text{Precision} &= \frac{|\mathrm{TP}|}{N_X}, &
\text{Recall}    &= \frac{|\mathrm{TP}'|}{N_Y}.
\end{align}
The harmonic mean yields the F1-score:
\begin{equation}
\mathrm{F1}(\tau) = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}.
\label{eq:f1_score}
\end{equation}

Unlike CD, F1-score is threshold-dependent and interpretable as a binary classification metric: it reflects the fraction of points within an acceptable error margin (e.g., $\tau = 1\,\text{mm}$ for facial geometry), thus aligning with perceptual or clinical relevance. A high F1-score indicates strong overlap in support—critical when evaluating whether fine facial features (e.g., nasal alae, lip contours) are preserved.

\paragraph{Why These Metrics?}  
- \textbf{Chamfer distance} provides a smooth, global scalar error suitable for optimization and ranking; it is less sensitive to outliers than Hausdorff distance and avoids the combinatorial complexity of exact correspondence.
- \textbf{F1-score} offers a human-interpretable performance gauge at clinically meaningful tolerances (e.g., sub-millimeter accuracy for surgical planning), and its symmetry ensures fairness when neither point set is strictly “ground truth” (e.g., in cross-method comparisons).
- Together, they mitigate each other’s weaknesses: CD may be dominated by dense regions, while F1-score ignores magnitude beyond $\tau$; using both gives a balanced view of *accuracy* (CD) and *completeness* (F1).

In our experiments, we report CD (in mm$^2$ or mm) and F1-score at $\tau = 0.5\,\text{mm}$, $1.0\,\text{mm}$, and $2.0\,\text{mm}$ to assess robustness across error scales.

\section{Experiments}
\subsection{Experimental Setup}

\textbf{Data Acquisition.} We conducted facial data collection in a clinical setting involving four healthy subjects (three females and one male, aged 20--30) with no craniofacial abnormalities. To establish high-fidelity ground truth, 3DMD photogrammetry was performed for each subject under the supervision of professional medical staff. Concurrently, video sequences were captured using a Realme Neo7 smartphone equipped with a Sony IMX882 sensor. The videos were recorded at 1080P resolution ($1920 \times 1080$) and 60 fps. During acquisition, subjects maintained a neutral expression while allowing for natural micro-expressions. To ensure optimal geometric reconstruction and prevent artifacts, all subjects were captured without occlusions (e.g., glasses, jewelry, or makeup). The camera followed a circular trajectory at a distance of 0.5--1.0 m under controlled indoor lighting.


\begin{table}[h]
\centering
\caption{Video Data Comparison}
\label{tab:data_comparison}
\begin{adjustbox}{width=\linewidth,center}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Person} & \textbf{Video Time (s)} & \textbf{Video Size (MB)} & \textbf{3DMD Image Size (MB)} & \textbf{3DMD Mesh Size (MB)} \\
\hline
\textbf{cfj} & 50 & 175 & 8.80 & 12.8 \\
\textbf{lst} & 39 & 138 & 9.29 & 11.9 \\
\textbf{szq} & 34 & 120 & 9.19 & 12.4 \\
\textbf{yhx} & 52 & 183 & 8.80 & 14.8 \\
\hline
\end{tabular}
\end{adjustbox}
\end{table}

\textbf{Preprocessing and SfM.} Video frames were extracted via \texttt{ffmpeg} using uniform temporal sampling. The frame extraction frequency was treated as a primary independent variable for subsequent ablation studies. Structure-from-Motion (SfM) was implemented using the COLMAP GUI, where we systematically evaluated various feature extraction and matching algorithms. 

\textbf{3D Reconstruction and Alignment.} All reconstruction algorithms were executed on an NVIDIA GeForce RTX 4090 GPU (24GB VRAM). Given the scale ambiguity inherent in monocular SfM and the lack of pixel-level alignment between the 3DMD reference and the mobile-captured data, standard Iterative Closest Point (ICP) methods were prone to scale mismatch. 
While dense point cloud registration is common in 3D face analysis, a growing body of work demonstrates that sparse, anatomically stable landmarks are sufficient—and often preferable—for robust alignment under expression variation~\cite{mai2024reliable,zhu2009unsupervised,kazemi2014one}.

Theoretically, rigid transformation in $\mathbb{R}^3$ is fully determined by three non-collinear point correspondences~\cite{horn1987closed}. In practice, however, stability under noise and non-rigidity (e.g., smiling) requires landmarks to be both \textit{anatomically fixed} and \textit{visually discriminable}. 
Mai~et~al.~\cite{mai2024reliable} systematically evaluate reference regions for 3D smile alignment and identify the \textit{nose}, \textit{forehead}, and \textit{chin} as reliable anchors—precisely because they exhibit minimal deformation during expression changes. 
Notably, the subnasale (base of the nasal septum) and exocanthions (outer eye corners) are highlighted as highly stable fiducials across posed and natural smiles.

Building on this, we select five key landmarks: bilateral exocanthions ($L_{\text{eye}}, R_{\text{eye}}$), subnasale ($L_{\text{nose}}$), and bilateral cheilions ($L_{\text{mouth}}, R_{\text{mouth}}$). Although only three non-collinear points are mathematically necessary, the inclusion of five provides redundancy against occlusion or detection error while maintaining sparsity. 
This configuration spans the upper, mid, and lower facial thirds, ensuring full spatial coverage—a design principle echoed in classical anthropometry~\cite{chen2022face,sagonas2013300} and modern 3D alignment pipelines~\cite{dan2020joint,bulat20173d}.

Empirically, Zhu~and~Van~Gool~\cite{zhu2009unsupervised} show that non-rigid face alignment can be initialized from as few as 3–5 landmarks using thin-plate splines, while Kazemi~and~Sullivan~\cite{kazemi2014one} achieve real-time 2D alignment with an ensemble of regression trees trained on just 68 points—many of which are redundant for coarse pose estimation. 

Our choice thus balances minimalism, anatomical validity, and compatibility with standard facial coding systems (e.g., FACS), enabling accurate CloudCompare-based ICP initialization without reliance on dense texture or mesh topology.Consequently, we employed a point-to-point manual alignment strategy in CloudCompare to calibrate the scale and orientation of the reconstructed point clouds against the 3DMD reference.


\subsection{Effect of Frame Rate on Reconstruction Quality}
Table~\ref{tab:reconstruction_metrics_combined} compares reconstruction accuracy at FPS=1 and FPS=3 using precision, recall, and F-score under geometric tolerances from $1$ to $4$ mm.

At FPS=1, all methods achieve perfect recall ($1.0000$) only at $\geq 2$ mm, indicating limited coverage of fine-scale geometry. 
In contrast, at FPS=3, recall reaches $1.0000$ even at $1$ mm for three out of four methods (\textit{lst}, \textit{szq}, \textit{yhx}), demonstrating significantly improved scene coverage due to denser temporal sampling.

Precision also improves markedly with higher FPS. For instance, \textit{lst-fps3} achieves a precision of $0.3512$ at $1$ mm—over three times higher than \textit{lst-fps1} ($0.1086$). 
Similarly, \textit{yhx-fps3} attains an F-score of $0.5907$ at $1$ mm, compared to $0.2899$ for its FPS=1 counterpart, reflecting enhanced localization accuracy.

Notably, both \textit{lst-fps3} and \textit{yhx-fps3} achieve perfect precision and recall at $4$ mm, while maintaining high F-scores ($>0.95$) even at $2$ mm. 
This confirms that increasing frame rate not only boosts completeness but also enhances geometric fidelity—critical for applications requiring sub-centimeter accuracy, such as clinical facial assessment or virtual try-on systems.
% 在导言区确保包含以下宏包

\begin{table}[htbp]
    \centering
    \caption{Evaluation of reconstruction accuracy at tolerance thresholds of 1–4 mm for two frame rates: FPS=1 and FPS=3. Each subtable reports precision (p), recall (r), and F-score (f) for four methods. Higher values indicate better alignment with the ground-truth point cloud.}
    \label{tab:reconstruction_metrics_combined}
    
    \begin{subtable}[t]{\linewidth}
        \centering
        \caption{FPS = 1}
        \begin{tabular}{l|cccc}
            \toprule
            & \textbf{1 mm} & \textbf{2 mm} & \textbf{3 mm} & \textbf{4 mm} \\
            \midrule
            \textbf{cfj-fps1} & 
            \makecell{p: 0.0165 \\ r: 0.6512 \\ f: 0.0322} &
            \makecell{p: 0.1724 \\ r: 1.0000 \\ f: 0.2941} &
            \makecell{p: 0.2647 \\ r: 1.0000 \\ f: 0.4186} &
            \makecell{p: 0.7222 \\ r: 1.0000 \\ f: 0.8387} \\
            \addlinespace[0.5em]
            \textbf{lst-fps1} & 
            \makecell{p: 0.1086 \\ r: 0.2791 \\ f: 0.1564} &
            \makecell{p: 0.3881 \\ r: 1.0000 \\ f: 0.5591} &
            \makecell{p: 0.8333 \\ r: 1.0000 \\ f: 0.9091} &
            \makecell{p: 1.0000 \\ r: 1.0000 \\ f: 1.0000} \\
            \addlinespace[0.5em]
            \textbf{szq-fps1} & 
            \makecell{p: 0.1586 \\ r: 0.2188 \\ f: 0.1698} &
            \makecell{p: 0.4289 \\ r: 1.0000 \\ f: 0.5743} &
            \makecell{p: 0.7810 \\ r: 1.0000 \\ f: 0.9044} &
            \makecell{p: 0.9470 \\ r: 1.0000 \\ f: 0.9899} \\
            \addlinespace[0.5em]
            \textbf{yhx-fps1} & 
            \makecell{p: 0.2000 \\ r: 0.5263 \\ f: 0.2899} &
            \makecell{p: 0.5357 \\ r: 1.0000 \\ f: 0.6977} &
            \makecell{p: 0.8846 \\ r: 1.0000 \\ f: 0.9388} &
            \makecell{p: 0.9149 \\ r: 1.0000 \\ f: 0.9556} \\
            \bottomrule
        \end{tabular}
    \end{subtable}
    
    \vspace{1.5em} % 增加子表间间距
    
    \begin{subtable}[t]{\linewidth}
        \centering
        \caption{FPS = 3}
        \begin{tabular}{l|cccc}
            \toprule
            & \textbf{1 mm} & \textbf{2 mm} & \textbf{3 mm} & \textbf{4 mm} \\
            \midrule
            \textbf{cfj-fps3} & 
            \makecell{p: 0.2000 \\ r: 0.5263 \\ f: 0.2899} &
            \makecell{p: 0.5357 \\ r: 1.0000 \\ f: 0.6977} &
            \makecell{p: 0.8846 \\ r: 1.0000 \\ f: 0.9388} &
            \makecell{p: 1.0000 \\ r: 1.0000 \\ f: 1.0000} \\
            \addlinespace[0.5em]
            \textbf{lst-fps3} & 
            \makecell{p: 0.3512 \\ r: 1.0000 \\ f: 0.5198} &
            \makecell{p: 0.6939 \\ r: 1.0000 \\ f: 0.8193} &
            \makecell{p: 0.9667 \\ r: 1.0000 \\ f: 0.9831} &
            \makecell{p: 1.0000 \\ r: 1.0000 \\ f: 1.0000} \\
            \addlinespace[0.5em]
            \textbf{szq-fps3} & 
            \makecell{p: 0.3164 \\ r: 1.0000 \\ f: 0.4807} &
            \makecell{p: 0.5256 \\ r: 1.0000 \\ f: 0.6891} &
            \makecell{p: 0.7742 \\ r: 1.0000 \\ f: 0.8727} &
            \makecell{p: 0.9500 \\ r: 1.0000 \\ f: 0.9744} \\
            \addlinespace[0.5em]
            \textbf{yhx-fps3} & 
            \makecell{p: 0.4192 \\ r: 1.0000 \\ f: 0.5907} &
            \makecell{p: 0.6346 \\ r: 1.0000 \\ f: 0.7765} &
            \makecell{p: 0.9184 \\ r: 1.0000 \\ f: 0.9574} &
            \makecell{p: 1.0000 \\ r: 1.0000 \\ f: 1.0000} \\
            \bottomrule
        \end{tabular}
    \end{subtable}
\end{table}

\subsection{Ablation Study}
\paragraph{FPS Ablation}
We conduct an ablation study to evaluate the impact of input frame rate on the quality of sparse 3D reconstruction using COLMAP~\cite{colmap}. 
Given a fixed video sequence, we extract keyframes at varying frame rates $\{0.6, 0.8, 1.0, 3, 5 ,10\}$ FPS and feed them into COLMAP for structure from motion (SfM). 
As shown in Figure~\ref{fig:cfj-fps_ablation}, the reconstructed camera trajectories (red wireframes) and point clouds (gray dots) exhibit significant improvements with increasing frame density.

At low FPS (e.g., 0.6 FPS), the trajectory is sparse and discontinuous, leading to incomplete coverage of the subject and poor geometric consistency. 
As FPS increases to 1–3 FPS, more camera poses are recovered, forming a smoother trajectory and denser point cloud around the head and upper body.
At 5 and 10 FPS, the trajectory becomes highly regular and closed-loop, enabling robust loop closure detection and full-body reconstruction. 
At the same time, we also tried a smaller fps (<0.5fps), but because the overlap between two adjacent photos was too small, it was not conducive to finding common features between two images using colmap, and could not be visually represented in the result image.
Notably, the number of reconstructed 3D points scales isn't linearly with FPS. As the frame rate increases, the number of adjacent image overlap points increases. This can be explained by connectivity theory. When the frame rate increases from 1 to 3, the number of common feature points found shows a significant improvement. From 3 to 5, the improvement decreases slightly, and then further decreases at 10. This demonstrates that the contribution of overlapping regions to the finding of common features first increases and then decreases.

This confirms that higher frame rates improve both geometric accuracy and completeness of SfM results, especially for dynamic scenes involving human motion. 
Our findings suggest that for high-fidelity reconstruction tasks, a minimum of 0.6 FPS is recommended to ensure sufficient spatial-temporal redundancy.

\begin{table}[htbp]
    \centering
    \caption{Reconstruction metrics across different frame rates.}
    \label{tab:fps_metrics}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc}
        \toprule
        \textbf{FPS} & \textbf{CFJ Image Num} & \textbf{CFJ Point Num} & \textbf{LST Image Num} & \textbf{LST Point Num} \\
        \midrule
        0.6 & 30   & 3206   & 13   & 3461   \\
        0.8 & 40   & 8197   & 20   & 6159   \\
        1   & 50   & 10986  & 25   & 8287   \\
        3   & 150  & 83635  & 118  & 68317  \\
        5   & 250  & 123774 & 196  & 103762 \\
        10  & 500  & 227780 & 393  & 191944 \\
        \bottomrule
    \end{tabular}
    }
\end{table}

Table~\ref{tab:fps_metrics} quantifies how input frame rate influences the scale of sparse reconstruction for two pipelines: \textit{CFJ} (our method) and \textit{IST} (baseline). 
As the frame rate increases from $0.6$ to $10$ FPS, both methods process more images and recover significantly more 3D points, reflecting greater scene coverage and feature overlap.

Notably, \textit{CFJ} consistently utilizes more input frames than \textit{IST} at each FPS setting (e.g., 500 vs. 393 images at 10 FPS), suggesting a higher tolerance to redundant or near-duplicate views. 
This directly translates into a denser point cloud: at 10 FPS, \textit{CFJ} reconstructs $227{,}780$ points compared to $191{,}944$ from \textit{IST}, a relative gain of $\sim18.7\%$. 
The advantage is even more pronounced at low frame rates (e.g., $+13.6\%$ at 0.6 FPS), indicating that \textit{CFJ} better leverages sparse observations through improved feature matching or robust pose initialization.

Moreover, the near-linear growth in point count with respect to image count—approximately $N_p \approx 450 \cdot N_{\text{img}}$ for \textit{CFJ}—demonstrates consistent scalability across sampling densities. 
These results confirm that our pipeline not only benefits from higher temporal resolution but also achieves higher reconstruction efficiency under data-scarce conditions, a critical property for real-world deployment where high-FPS capture may be unavailable.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/251012Hospital-cfj-oneround_fps0.6_result.png}
        \caption{cfj-0.6fps}
        \label{fig:cfj-0.6fps}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/251012Hospital-cfj-oneround_fps0.8_result.png}
        \caption{cfj-0.8fps}
        \label{fig:cfj-0.8fps}
    \end{subfigure}

    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/251012Hospital-cfj-oneround_fps1_result.png}
        \caption{cfj-1.0fps}
        \label{fig:cfj-1.0fps}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/251012Hospital-cfj-oneround_fps3_result.png}
        \caption{cfj-3.0fps}
        \label{fig:cfj-3.0fps}
    \end{subfigure}

    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/251012Hospital-cfj-oneround_fps5_result.png}
        \caption{cfj-5.0fps}
        \label{fig:cfj-5.0fps}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/251012Hospital-cfj-oneround_fps10_result.png}
        \caption{cfj-10.0fps}
        \label{fig:cfj-10.0fps}
    \end{subfigure}

    \caption{
        3D reconstruction results from COLMAP using video frames sampled at different frame rates.
        Red wireframes indicate estimated camera poses; gray points form the sparse point cloud.
        Higher frame rates yield denser reconstructions and more consistent trajectory estimation.
    }
    \label{fig:cfj-fps_ablation}
\end{figure}


\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/251012Hospital-lst-oneround_fps0.6_result.png}
        \caption{lst-0.6fps}
        \label{fig:lst-0.6fps}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/251012Hospital-lst-oneround_fps0.8_result.png}
        \caption{lst-0.8fps}
        \label{fig:lst-0.8fps}
    \end{subfigure}

    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/251012Hospital-lst-oneround_fps1_result.png}
        \caption{lst-1.0fps}
        \label{fig:lst-1.0fps}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/251012Hospital-lst-oneround_fps3_result.png}
        \caption{lst-3.0fps}    
        \label{fig:lst-3.0fps}
    \end{subfigure}

    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/251012Hospital-lst-oneround_fps5_result.png}
        \caption{lst-5.0fps}
        \label{fig:lst-5.0fps}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/251012Hospital-lst-oneround_fps10_result.png}
        \caption{lst-10.0fps}
        \label{fig:lst-10.0fps}
    \end{subfigure}

    \caption{
        3D reconstruction results from COLMAP using video frames sampled at different frame rates.
        Red wireframes indicate estimated camera poses; gray points form the sparse point cloud.
        Higher frame rates yield denser reconstructions and more consistent trajectory estimation.
    }
    \label{fig:lst-fps_ablation}
\end{figure}

\paragraph{Matching Strategies in COLMAP Ablation}
COLMAP~\cite{colmap} supports two primary feature matching schemes for structure-from-motion (SfM): \textit{exhaustive matching} and \textit{sequential matching}. 
In exhaustive matching, every image $I_i$ is matched against all other images $\{I_j\}_{j \neq i}$, resulting in a complete pairwise matching graph with $\mathcal{O}(N^2)$ edges for $N$ input images. 
This strategy maximizes the chance of finding overlapping views, especially beneficial for unordered or multi-view datasets with complex camera trajectories.

In contrast, sequential matching assumes a temporal ordering of images (e.g., from video) and only matches each image $I_t$ to its $k$ nearest neighbors in time: $\{I_{t-\delta}, \dots, I_{t+\delta}\}$ for some window size $\delta$. 
The number of matching pairs scales linearly as $\mathcal{O}(N \cdot \delta)$, drastically reducing computational cost. 
Formally, the match set under sequential matching is:
\[
\mathcal{M}_{\text{seq}} = \bigcup_{t=1}^{N} \big\{ (I_t, I_{t'}) \mid |t - t'| \leq \delta,\, t' \neq t \big\},
\]
whereas exhaustive matching yields:
\[
\mathcal{M}_{\text{exh}} = \big\{ (I_i, I_j) \mid 1 \leq i < j \leq N \big\}.
\]

While sequential matching is efficient and well-suited for video streams with smooth motion, it may fail to establish loop closures or connect distant but visually similar views. 
Exhaustive matching, though computationally expensive, provides richer connectivity and often leads to more complete reconstructions—especially when frame rates are low or motion is non-linear. 
In our ablation studies (Fig\ref{fig:colmap_match_ablation}), we adopt exhaustive matching to ensure fair comparison across varying FPS settings, where temporal locality alone is insufficient for robust reconstruction.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/exhaustive-25.png}
        \caption{exhaustive-25fps}
        \label{fig:exhaustive-25}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/exhaustive-50.png}
        \caption{exhaustive-50fps}
        \label{fig:exhaustive-50}
    \end{subfigure}

    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/sequential-25.png}
        \caption{sequential-25fps}
        \label{fig:sequential-25}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{images/sequential-50.png}
        \caption{sequential-50fps}
        \label{fig:sequential-50}
    \end{subfigure}


    \caption{
        We conducted ablation experiments on the same sample using both matching methods. We found that when the sample itself is a continuous segment extracted from the video, choosing the sequential method is physically justified, eliminating the need for redundant greedy matching. In terms of final reconstruction quality, there was no significant difference between the two methods.
    }
    \label{fig:colmap_match_ablation}
\end{figure}

\section{Conclusion & Contribution}
We have created a pipeline PFV-3D for 3D face reconstruction by capturing videos with a mobile phone and uploading them to the cloud. The mesh quality reconstructed using this method is essentially the same as that reconstructed using 3DMD at a scale of approximately 2mm, and it consumes fewer resources and has better environmental requirements than 3DMD. In the future, we can explore dividing face reconstruction into regions of interest, weighting the importance of different facial regions, and performing small-scale, fine-grained reconstructions of specific regions for specific tasks. Our method holds promise for applications in medical mechanism research, lesion detection, and digital human development.

We do contribution equally in this project, and Chunyu He finish the final report on his own.

\bibliographystyle{unsrtnat}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Supplementary Details on Fisheye Camera Modeling}
\label{app:supp}

\subsection{Back-Projection via Newton–Raphson Iteration}
\label{app:backproj}

Given a pixel coordinate $(x, y)$ and intrinsic parameters $\boldsymbol{\theta}_{\mathrm{int}} = [f_x, f_y, c_x, c_y, k_1, k_2, k_3, k_4]^\top$, the back-projection to a unit ray $\mathbf{u} \in \mathbb{R}^3$ proceeds as follows:

\begin{algorithm}[H]
\caption{BackProject$(x, y, \boldsymbol{\theta}_{\mathrm{int}})$}
\label{alg:backproj}
\DontPrintSemicolon
$\tilde{x} \gets \dfrac{x - c_x}{f_x},\quad 
 \tilde{y} \gets \dfrac{y - c_y}{f_y}$\;
$r \gets \sqrt{\tilde{x}^2 + \tilde{y}^2},\quad 
\phi \gets \atan(\tilde{y}, \tilde{x})$\;
$\theta^{(0)} \gets r$ \tcp*[r]{initial guess}
\For{$n = 0$ \KwTo $N_{\text{iter}}-1$}{
    $f(\theta^{(n)}) \gets \theta^{(n)} + \sum_{\ell=1}^{4} k_\ell \left(\theta^{(n)}\right)^{2\ell+1}$\;
    $f'(\theta^{(n)}) \gets 1 + \sum_{\ell=1}^{4} (2\ell+1) k_\ell \left(\theta^{(n)}\right)^{2\ell}$\;
    $\theta^{(n+1)} \gets \theta^{(n)} - \dfrac{f(\theta^{(n)}) - r}{f'(\theta^{(n)})}$\;
    \lIf{$|\theta^{(n+1)} - \theta^{(n)}| < \epsilon_\theta$}{\textbf{break}}
}
$\mathbf{u} \gets 
\begin{bmatrix}
\sin\theta^{(n+1)} \cos\phi \\
\sin\theta^{(n+1)} \sin\phi \\
\cos\theta^{(n+1)}
\end{bmatrix}$\;
\Return $\mathbf{u}$\;
\end{algorithm}

\noindent
The iteration converges quadratically for $|\theta| < \pi/2$ and linearly near $\theta \to \pi$; in practice, $N_{\text{iter}} = 5$ suffices for sub-pixel accuracy ($\epsilon_\theta = 10^{-8}$ rad). This routine is used in BA (Alg.~\ref{alg:kb_ba}) to compute residuals and Jacobians.

\subsection{Analytic Jacobians for KB Reprojection Error}
\label{app:jacobian}

Let $\mathbf{r} = [r_x, r_y]^\top = \proj(\mathbf{X}_c; \boldsymbol{\theta}_{\mathrm{int}}) - \mathbf{x}$ be the reprojection residual. Denote $\mathbf{X}_c = [x_c, y_c, z_c]^\top$, $\rho = \|\mathbf{X}_c\|$, and $\mathbf{u} = \mathbf{X}_c / \rho$. Define:
\begin{align}
    \theta &= \arccos(u_z), &
    r &= f(\theta) = \theta + \sum_{\ell=1}^4 k_\ell \theta^{2\ell+1}, \\
    \phi &= \atan(u_y, u_x), &
    s &= \sin\theta,\; c = \cos\theta.
\end{align}
Then the partial derivatives are:

\paragraph{W.r.t. distortion coefficients $\mathbf{k} = [k_1, k_2, k_3, k_4]^\top$:}
\begin{equation}
    \frac{\partial \mathbf{r}}{\partial k_\ell}
    = 
    \begin{bmatrix}
        f_x \cdot \dfrac{\partial r}{\partial k_\ell} \cos\phi \\
        f_y \cdot \dfrac{\partial r}{\partial k_\ell} \sin\phi
    \end{bmatrix},
    \quad
    \frac{\partial r}{\partial k_\ell} = \theta^{2\ell+1}.
    \label{eq:dr_dk}
\end{equation}

\paragraph{W.r.t. rotation vector $\boldsymbol{\omega}$:}
Let $\mathbf{R} = \exp([\boldsymbol{\omega}]_\times)$ and $\delta\mathbf{R} = \frac{\partial \mathbf{R}}{\partial \omega_i} \mathbf{X}$. Then:
\begin{equation}
    \frac{\partial \mathbf{r}}{\partial \omega_i}
    = \frac{\partial \mathbf{r}}{\partial \mathbf{u}} 
      \frac{\partial \mathbf{u}}{\partial \mathbf{X}_c}
      \frac{\partial \mathbf{X}_c}{\partial \omega_i}
    = \mathbf{J}_{\mathrm{proj}} \cdot \mathbf{J}_{\mathrm{norm}} \cdot \left( [\mathbf{X}]_\times \mathbf{R}^\top \mathbf{u} \right),
    \label{eq:dr_domega}
\end{equation}
where
\begin{align}
    \mathbf{J}_{\mathrm{proj}} &= 
    \begin{bmatrix}
        f_x \cos\phi \cdot \dfrac{dr}{d\theta} & -f_x r \sin\phi \\
        f_y \sin\phi \cdot \dfrac{dr}{d\theta} &  f_y r \cos\phi
    \end{bmatrix}, \\
    \frac{dr}{d\theta} &= 1 + \sum_{\ell=1}^4 (2\ell+1) k_\ell \theta^{2\ell}, \\
    \mathbf{J}_{\mathrm{norm}} &= \frac{1}{\rho}
    \left( \mathbf{I} - \mathbf{u} \mathbf{u}^\top \right).
\end{align}

\paragraph{W.r.t. 3D point $\mathbf{X}$:}
\begin{equation}
    \frac{\partial \mathbf{r}}{\partial \mathbf{X}}
    = \mathbf{J}_{\mathrm{proj}} \, \mathbf{J}_{\mathrm{norm}} \, \mathbf{R}.
    \label{eq:dr_dX}
\end{equation}

These Jacobians enable exact first-order optimization in KB-BA without numerical differentiation. Implementation in PyTorch is straightforward using `torch.autograd.Function` with custom backward pass.

\subsection{Comparison with Pinhole Model}
\label{app:pinhole_vs_fisheye}

The standard pinhole projection assumes:
\begin{equation}
    r_{\text{pin}} = f \tan\theta, \quad \text{so} \quad 
    \frac{dr_{\text{pin}}}{d\theta} = f \sec^2\theta = f (1 + \tan^2\theta) = f \left(1 + \frac{r_{\text{pin}}^2}{f^2}\right).
    \label{eq:pinhole_dr}
\end{equation}
In contrast, the KB model uses a polynomial $r(\theta)$ with bounded derivative (since $|dr/d\theta| \leq 1 + \sum |(2\ell+1)k_\ell| \theta^{2\ell}$), avoiding the singularity at $\theta \to \pi/2$ inherent to pinhole models. This is critical for fisheye lenses where $\theta \in [0, \pi]$ (e.g., iPhone Ultra Wide: FoV $120^\circ \Rightarrow \theta_{\max} \approx 2.09$ rad).

Table~\ref{tab:model_comparison} summarizes key differences:

\begin{table}[h]
\centering
\caption{Pinhole vs. KB fisheye model properties.}
\label{tab:model_comparison}
\small
\begin{adjustbox}{width=\linewidth,center}
\begin{tabular}{lccc}
\toprule
Property & Pinhole & KB Fisheye & Advantage \\
\midrule
Projection law & $r = f \tan\theta$ & $r = \sum_{\ell=0}^4 a_\ell \theta^{2\ell+1}$ & Handles $\theta > \pi/2$ \\
Max FoV & $< 180^\circ$ (singularity) & $180^\circ$ (exact) & Full hemispherical coverage \\
Distortion params & None (ideal) & $k_1,\dots,k_4$ & Calibrates lens nonlinearity \\
Inverse projection & Closed-form: $\theta = \arctan(r/f)$ & Newton iteration (5 iters) & Slight overhead, but robust \\
BA Jacobian stability & Poor near $\theta \to \pi/2$ & Uniformly bounded & Better convergence \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

This justifies our choice of KB model for smartphone fisheye reconstruction pipelines.

\subsection{Calibration Protocol for Smartphone Fisheye Cameras}
\label{app:calib_protocol}

We calibrate intrinsic parameters using a planar checkerboard captured from multiple poses (min. 15 images). Steps:
\begin{enumerate}
    \item Detect corners with subpixel accuracy (OpenCV \texttt{findChessboardCornersSB}).
    \item Initialize $f_x, f_y, c_x, c_y$ via Zhang’s method~\cite{zhang2000flexible} on undistorted (assumed pinhole) points.
    \item Refine all 8 intrinsics ($f_x,f_y,c_x,c_y,k_1..k_4$) by minimizing reprojection error under KB model (Eq.~\ref{eq:ba_loss} with fixed $\mathbf{X}_j$, $\mathbf{R}^{(i)},\mathbf{t}^{(i)}$).
    \item Use Levenberg–Marquardt with damping $\lambda=10^{-3}$; converge when $\Delta < 10^{-6}$ pixels.
\end{enumerate}
Typical calibrated values for iPhone 14 Ultra Wide ($\sim$13mm equiv.):
\[
f_x = f_y \approx 385.2,\;
c_x = 1080,\; c_y = 720,\;
\mathbf{k} = [-0.182,\, 0.041,\, -0.008,\, 0.001]^\top.
\]
These yield mean reprojection error $< 0.3$ px on validation images.

% Optional: Add citation key if needed
% \bibliographystyle{plain}
% \bibliography{reference}
\end{document}