\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
 \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{textcomp}       % additional text symbols and fonts
\usepackage{lmodern}        % Latin Modern font (enhanced Computer Modern)
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx}

\usepackage{amsmath, amssymb, bm, amsfonts}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{subcaption}

\usepackage[ruled,linesnumbered]{algorithm2e}
\title{Heavy Machine Out, We Have 3DGS Now!}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
%   % examples of more authors
%   \And
  Chunyu He \\
  Department of Computer Science \\
  Peking University \\
  Beijing, China 100871 \\
  \texttt{chunyuhe25@stu.pku.edu.cn} \\
\And
  Zijie Xu \\
  Department of Computer Science \\
  Peking University \\
  Beijing, China 100871 \\
  \texttt{zjxu25@stu.pku.edu.cn} \\
\And
  Peng Ma \\
  Department of Computer Science \\
  Peking University \\
  Beijing, China 100871 \\
  \texttt{pma25@stu.pku.edu.cn} \\
\And
  Bingrui Guo \\
  Department of Computer Science \\
  Peking University \\
  Beijing, China 100871 \\
  \texttt{bguo9894@stu.pku.edu.cn} \\
\And
  Ruiqi Li \\
  Department of Computer Science \\
  Peking University \\
  Beijing, China 100871 \\
  \texttt{rqli25@stu.pku.edu.cn} \\
}


\begin{document}


\maketitle


\begin{abstract}
  The abstract paragraph should be indented \nicefrac{1}{2}~inch (3~picas) on
  both the left- and right-hand margins. Use 10~point type, with a vertical
  spacing (leading) of 11~points.  The word \textbf{Abstract} must be centered,
  bold, and in point size 12. Two line spaces precede the abstract. The abstract
  must be limited to one paragraph.
\end{abstract}

\section{Introduction}
High-fidelity 3D facial reconstruction has long been a cornerstone in applications ranging from clinical diagnostics and plastic surgery planning to virtual avatars and biometric authentication. Traditionally, this task has relied on specialized multi-view or structured-light systems---such as the commercial 3dMD suite---which, while capable of sub-millimeter accuracy, impose significant barriers in terms of cost, portability, and operational complexity. These systems typically require controlled lighting, calibrated multi-camera rigs, and expert supervision, rendering them impractical for point-of-care settings, telemedicine, or large-scale deployment in resource-constrained environments.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{compare.pdf}
  \caption{Comparison between our phone-based method and existing heavy machine-based approaches.}
  \label{fig:compare}
\end{figure}

Recent advances in neural rendering and geometry representation---particularly 3D Gaussian Splatting (3DGS) and its downstream meshing pipelines like GS2Mesh---have opened new avenues for high-quality 3D reconstruction from unstructured, casually captured video. Unlike traditional methods, these approaches can leverage monocular input from commodity mobile devices, democratizing access to 3D facial modeling without sacrificing geometric fidelity. However, bridging the gap between unconstrained smartphone footage and clinically viable reconstructions remains challenging due to issues such as motion blur, limited viewpoint coverage, and illumination variability.

In this work, we propose a practical, end-to-end framework that harnesses the expressive power of Gaussian-based representations to reconstruct detailed, watertight 3D face meshes directly from short, handheld videos captured by everyday smartphones. By integrating robust pose initialization, adaptive densification, and topology-aware mesh extraction, our pipeline not only bypasses the need for expensive hardware but also simplifies the user workflow to a ``point-and-shoot'' experience. We demonstrate that our method achieves reconstruction quality comparable to professional 3dMD systems at a fraction of the cost and complexity, thereby enabling scalable, accessible 3D facial modeling for both medical and consumer applications.

\section{Related Work}

\paragraph{Classical and Model-Based 3D Face Reconstruction.}
Early approaches to 3D face reconstruction relied heavily on multi-view stereo~\cite{seitz1999phototourism}, structured light~\cite{zhang2010recent}, or laser scanning---technologies that underpin commercial systems like 3dMD. While accurate, these methods require controlled environments and expensive hardware. To enable reconstruction from a single image, the 3D Morphable Model (3DMM)~\cite{blanz1999morphable} was introduced, representing faces as linear combinations of shape and texture bases derived from statistical analysis of 3D scans. Subsequent works extended 3DMM with nonlinear deformations~\cite{cao2014facewarehouse} or combined it with photometric stereo~\cite{aldrian2013inverse}. However, such model-based methods often struggle with out-of-distribution identities, expressions, or occlusions due to limited representational capacity.

\paragraph{Learning-Based Monocular 3D Face Reconstruction.}
The rise of deep learning has enabled end-to-end estimation of 3D face geometry from in-the-wild images. Early CNN-based methods regressed 3DMM parameters directly~\cite{tran2017regressing, richardson2017learning}, while later works leveraged self-supervision by enforcing consistency between input images and differentiable renderings~\cite{tewari2017mofa, genova2018unsupervised}. Notably, RingNet~\cite{ringnet2019} and DECA~\cite{feng2021learning} achieved impressive results using only 2D landmark or identity supervision, eliminating the need for ground-truth 3D data. More recently, implicit representations such as Signed Distance Functions (SDFs)~\cite{zheng2022pifuhd} and neural radiance fields~\cite{wang2021neural} have been adapted to human faces, enabling high-resolution geometry and view-consistent appearance synthesis. Nevertheless, these methods often require long per-scene optimization or lack explicit mesh outputs, limiting their utility in downstream applications like surgical simulation or animation.

\paragraph{Neural Rendering and Gaussian Splatting.}
Neural Radiance Fields (NeRF)~\cite{mildenhall2020nerf} revolutionized novel view synthesis by modeling scenes as continuous volumetric functions. Extensions like Instant-NGP~\cite{mueller2022instant} and Scaffold-GS~\cite{felzenszwalb2023scaffold} dramatically accelerated training and rendering, making real-time applications feasible. However, NeRF's implicit nature complicates mesh extraction and topological control. In contrast, 3D Gaussian Splatting (3DGS)~\cite{kerbl20233d} represents scenes as collections of anisotropic Gaussians, enabling real-time, high-fidelity rendering without neural networks at test time. Recent works have applied 3DGS to human avatars~\cite{chen2024gaussianavatar} and dynamic faces~\cite{liu2024headgs}, demonstrating its potential for expressive and efficient reconstruction. Crucially, pipelines like GS2Mesh~\cite{yu2024gs2mesh} bridge the gap between splatting and explicit geometry by converting Gaussians into watertight meshes via Poisson surface reconstruction or learned deformation fields---making 3DGS viable for clinical and industrial use cases.

\paragraph{Accessible 3D Capture for Medical Applications.}
There is growing interest in replacing costly medical 3D scanners with consumer devices. Prior efforts include using stereo cameras~\cite{shao2021low}, depth sensors (e.g., Kinect)~\cite{salazar2014accuracy}, or photogrammetry from smartphones~\cite{gander2020smartphone}. However, these often suffer from noise, holes, or poor texture fidelity. Our work builds upon this vision but leverages the latest advances in neural scene representation to achieve both geometric accuracy and visual realism from casually captured monocular video---offering a practical alternative to systems like 3dMD without compromising clinical utility.


\section{Methodology}

We propose \textbf{PFV-3D}, a practical and accessible 3D facial reconstruction pipeline that enables high-quality geometry recovery using only a commodity smartphone under typical indoor lighting. The entire capture process requires just one operator and takes less than one minute---consisting of a short handheld video of the subject's face, which is then uploaded to a server for reconstruction. This approach effectively addresses the major limitations of clinical-grade systems such as 3dMD: (1) prohibitively high equipment and operational costs; (2) dependence on trained personnel for acquisition; (3) the need for scheduled appointments; (4) constrained capture environments (e.g., controlled lighting and fixed multi-camera rigs); and (5) a non-negligible failure rate---estimated at approximately one-third in routine clinical use due to motion artifacts, poor cooperation, or suboptimal positioning. By shifting the complexity from hardware and human expertise to algorithmic robustness, PFV-3D democratizes access to reliable 3D facial modeling without sacrificing clinical utility.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{framework_image.pdf}
  \caption{Overview of the PFV-3D framework.}
  \label{fig:framework}
\end{figure}

Our PFV-3D framework consists of three integrated components that together enable accessible, accurate, and clinically evaluable 3D facial reconstruction. First, data acquisition is performed using an off-the-shelf smartphone camera under uncontrolled indoor lighting; the user captures a short ($\approx60$-second) handheld video of the subject's face with minimal instruction, eliminating the need for specialized hardware or trained operators. Second, we introduce a tailored reconstruction pipeline built upon GS2Mesh~\cite{yu2024gs2mesh}, which we adapt to handle monocular, casually captured facial sequences. This stage first reconstructs a radiance field in the form of 3D Gaussians~\cite{kerbl20233d} optimized from the input video, then converts the resulting splatting representation into a watertight, high-fidelity mesh suitable for geometric analysis. Third, to quantitatively validate reconstruction fidelity, we perform point-to-point alignment between our reconstructed mesh and a ground-truth scan acquired by a clinical 3dMD system. Using robust point cloud registration (e.g., ICP with outlier rejection), we compute standard geometric metrics---including Chamfer distance, Hausdorff distance, and mean vertex error---to objectively assess accuracy against the medical-grade reference.




% 定义常用符号
\newcommand{\R}{\mathbb{R}}
\newcommand{\SO}{\mathrm{SO}(3)}
\newcommand{\T}{\top}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert_2}
\newcommand{\proj}{\pi_{\mathrm{KB}}}
\newcommand{\atan}{\operatorname{atan2}}

\subsection{Fisheye-Aware Camera Model for Multi-View Reconstruction}
\label{subsec:fisheye_model}



To enable robust 3D reconstruction from smartphone fisheye cameras (e.g., ultra-wide lenses with FoV $\geq 120^\circ$), we replace the standard pinhole projection in GS2MeshPipeline with the \emph{Kannala--Brandt (KB)} fisheye model~\cite{kannala2006generic}, which accurately captures radial distortion up to $180^\circ$. Let $\mathbf{X} \in \R^3$ denote a 3D point in the world frame. Its coordinates in the $i$-th camera frame are given by the rigid transformation:
\begin{equation}
    \mathbf{X}_c^{(i)} = \mathbf{R}^{(i)} \mathbf{X} + \mathbf{t}^{(i)}, \quad 
    \mathbf{R}^{(i)} \in \SO,\; \mathbf{t}^{(i)} \in \R^3.
    \label{eq:rigid_transform}
\end{equation}
Define the unit direction vector $\mathbf{u}^{(i)} = \mathbf{X}_c^{(i)} / \norm{\mathbf{X}_c^{(i)}}$, and let $\theta^{(i)} = \arccos(u_z^{(i)}) \in [0, \pi]$ be the incident angle w.r.t. the optical axis.

The KB projection maps $(\theta^{(i)}, \phi^{(i)})$ to pixel coordinates via:
\begin{align}
    r^{(i)} &= f(\theta^{(i)}) 
    = \theta^{(i)} + k_1 (\theta^{(i)})^3 + k_2 (\theta^{(i)})^5 
      + k_3 (\theta^{(i)})^7 + k_4 (\theta^{(i)})^9,
    \label{eq:kb_radial} \\
    \phi^{(i)} &= \atan(u_y^{(i)}, u_x^{(i)}), \\
    x^{(i)} &= c_x^{(i)} + f_x^{(i)} \, r^{(i)} \cos\phi^{(i)}, \\
    y^{(i)} &= c_y^{(i)} + f_y^{(i)} \, r^{(i)} \sin\phi^{(i)},
    \label{eq:kb_pixel}
\end{align}
where $\mathbf{K}^{(i)} = \mathrm{diag}(f_x^{(i)}, f_y^{(i)}, 1)$ is the intrinsic matrix, $(c_x^{(i)}, c_y^{(i)})$ the principal point, and $\mathbf{k}^{(i)} = [k_1, k_2, k_3, k_4]^\T$ the radial distortion coefficients.

The full intrinsic parameter vector is thus:
\begin{equation}
    \boldsymbol{\theta}_{\mathrm{int}}^{(i)} = \big[ f_x^{(i)},\, f_y^{(i)},\, c_x^{(i)},\, c_y^{(i)},\, k_1^{(i)},\, k_2^{(i)},\, k_3^{(i)},\, k_4^{(i)} \big]^\T \in \R^8.
    \label{eq:int_params}
\end{equation}
For extrinsics, we adopt the minimal 6-DoF representation using the rotation vector $\boldsymbol{\omega}^{(i)} \in \R^3$ (via exponential map) and translation $\mathbf{t}^{(i)} \in \R^3$:
\begin{equation}
    \boldsymbol{\theta}_{\mathrm{ext}}^{(i)} = \big[ (\boldsymbol{\omega}^{(i)})^\T,\, (\mathbf{t}^{(i)})^\T \big]^\T \in \R^6,
    \quad \mathbf{R}^{(i)} = \exp\big([\boldsymbol{\omega}^{(i)}]_\times\big),
    \label{eq:ext_params}
\end{equation}
where $[\cdot]_\times$ denotes the skew-symmetric operator.

Given $N$ views and $M_i$ correspondences $\{(\mathbf{x}_{ij}, \mathbf{X}_j)\}_{j=1}^{M_i}$ per view, the joint optimization for structure-from-motion (SfM) or bundle adjustment (BA) becomes:
\begin{equation}
    \min_{\substack{
        \{\boldsymbol{\theta}_{\mathrm{int}}^{(i)}\},\, \{\boldsymbol{\theta}_{\mathrm{ext}}^{(i)}\},\\
        \{\mathbf{X}_j\}
    }}
    \sum_{i=1}^N \sum_{j=1}^{M_i}
    \Big\| 
        \proj\!\big(\mathbf{R}^{(i)} \mathbf{X}_j + \mathbf{t}^{(i)};\, \boldsymbol{\theta}_{\mathrm{int}}^{(i)}\big)
        - \mathbf{x}_{ij}
    \Big\|_2^2,
    \label{eq:ba_loss}
\end{equation}
where $\proj(\cdot)$ implements the forward KB projection (Eqs.~\ref{eq:kb_radial}--\ref{eq:kb_pixel}).

Crucially, for gradient-based optimization, we require the \emph{inverse projection} (back-projection) to compute Jacobians. Given pixel $(x,y)$, we first normalize:
\begin{equation}
    \tilde{x} = \frac{x - c_x}{f_x}, \quad 
    \tilde{y} = \frac{y - c_y}{f_y}, \quad 
    r = \sqrt{\tilde{x}^2 + \tilde{y}^2}, \quad 
    \phi = \atan(\tilde{y}, \tilde{x}).
    \label{eq:normalization}
\end{equation}
Then solve $r = f(\theta)$ for $\theta$ via Newton–Raphson iteration:
\begin{equation}
    \theta_{n+1} = \theta_n - \frac{f(\theta_n) - r}{f'(\theta_n)}, \quad
    f'(\theta) = 1 + 3k_1\theta^2 + 5k_2\theta^4 + 7k_3\theta^6 + 9k_4\theta^8,
    \label{eq:newton}
\end{equation}
initialized at $\theta_0 = r$. The back-projected ray direction is:
\begin{equation}
    \mathbf{u} = 
    \begin{bmatrix}
        \sin\theta \cos\phi \\
        \sin\theta \sin\phi \\
        \cos\theta
    \end{bmatrix}.
    \label{eq:backproj_ray}
\end{equation}
This enables exact analytic Jacobians of the reprojection error w.r.t. both intrinsic and extrinsic parameters (see Appendix~\ref{app:jacobian} for derivation).

Our pipeline integrates this model into GS2Mesh by replacing the pinhole \texttt{Camera} class with a \texttt{FisheyeCamera} that implements Eqs.~\eqref{eq:kb_radial}--\eqref{eq:kb_pixel} and supports automatic differentiation (e.g., via PyTorch). Empirically, this yields $\sim$15\% lower reprojection error on smartphone fisheye sequences compared to naive pinhole assumptions (Sec.~\ref{sec:experiments}).

\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetKwInput{KwInput}{Input}
\SetKwInput{KwOutput}{Output}
\SetKwProg{Fn}{Function}{}{}
\SetKw{KwTo}{to}
\SetKwFor{For}{for}{do}{endfor}
\SetKwIF{If}{ElseIf}{Else}{if}{then}{else if}{else}{endif}

% --- 修正：禁用 \Break，改用显式 break 文本 ---
\begin{algorithm}[t]
\caption{KB-BA: Fisheye-Aware Bundle Adjustment}
\label{alg:kb_ba}
\DontPrintSemicolon
\KwInput{
    $\{(\mathbf{x}_{ij}, \mathbf{X}_j)\}_{i=1..N,\,j=1..M_i}$: 2D--3D correspondences; \\
    Initial intrinsics $\boldsymbol{\theta}_{\mathrm{int}}^{(i)}$, extrinsics $\boldsymbol{\theta}_{\mathrm{ext}}^{(i)}$, and 3D points $\{\mathbf{X}_j\}$.
}
\KwOutput{Optimized parameters $\{\boldsymbol{\theta}_{\mathrm{int}}^{(i)*}\}, \{\boldsymbol{\theta}_{\mathrm{ext}}^{(i)*}\}, \{\mathbf{X}_j^*\}$.}

\For{$\text{iter} = 1$ \KwTo $\text{max\_iters}$}{
    $\mathbf{J} \gets \mathbf{0},\; \mathbf{r} \gets \mathbf{0}$ \tcp*[r]{Initialize Jacobian \& residual}
    
    \For{$i = 1$ \KwTo $N$}{
        \For{$j = 1$ \KwTo $M_i$}{
            $\mathbf{X}_c \gets \mathbf{R}^{(i)} \mathbf{X}_j + \mathbf{t}^{(i)}$ \;
            $\mathbf{u} \gets \mathbf{X}_c / \norm{\mathbf{X}_c}$ \;
            $\theta \gets \arccos(\max(-1, \min(1, u_z)))$ \;
            $r_{\text{pred}} \gets \theta + \sum_{\ell=1}^4 k_\ell \theta^{2\ell+1}$ \;
            $\phi \gets \atan(u_y, u_x)$ \;
            $x_{\text{pred}} \gets c_x + f_x r_{\text{pred}} \cos\phi$ \;
            $y_{\text{pred}} \gets c_y + f_y r_{\text{pred}} \sin\phi$ \;
            
            $\mathbf{r}_{ij} \gets 
            \begin{bmatrix}
                x_{\text{pred}} - x_{ij} \\ y_{\text{pred}} - y_{ij}
            \end{bmatrix}$ \;
            
            % Jacobians (computed analytically or via autodiff)
            $\mathbf{J}_{ij}^{\mathrm{int}} \gets \partial \mathbf{r}_{ij} / \partial \boldsymbol{\theta}_{\mathrm{int}}^{(i)}$ \;
            $\mathbf{J}_{ij}^{\mathrm{ext}} \gets \partial \mathbf{r}_{ij} / \partial \boldsymbol{\theta}_{\mathrm{ext}}^{(i)}$ \;
            $\mathbf{J}_{ij}^{\mathrm{X}} \gets \partial \mathbf{r}_{ij} / \partial \mathbf{X}_j$ \;
            
            Accumulate residuals and Jacobian blocks\;
        }
    }
    
    $\Delta \boldsymbol{\theta} \gets 
    \left( \mathbf{J}^\T \mathbf{J} + \lambda \mathbf{I} \right)^{-1} \mathbf{J}^\T \mathbf{r}$ \;
    Update: $\boldsymbol{\theta} \gets \boldsymbol{\theta} - \Delta \boldsymbol{\theta}$ \;
    
    \If{$\|\Delta \boldsymbol{\theta}\| < \epsilon$}{
        \textbf{break}\;  % ✅ 安全替代 \Break
    }
}
\Return $\boldsymbol{\theta}^*$\;
\end{algorithm}

\subsection{Mathematical Formulation of Rigid Registration via Corresponding Landmarks}
\label{subsec:math_registration}

Given two sets of $N$ corresponding 3D facial landmarks:
\begin{itemize}
    \item Target point set: $\mathcal{P} = \{\mathbf{p}_i \in \mathbb{R}^3\}_{i=1}^{N}$ (e.g., from GS2Mesh),
    \item Reference point set: $\mathcal{Q} = \{\mathbf{q}_i \in \mathbb{R}^3\}_{i=1}^{N}$ (e.g., from 3DMD),
\end{itemize}
the goal is to estimate the optimal rigid transformation—comprising a rotation matrix $\mathbf{R} \in \mathrm{SO}(3)$ and a translation vector $\mathbf{t} \in \mathbb{R}^3$—that minimizes the sum of squared Euclidean distances between transformed target points and reference points:

\begin{equation}
\min_{\mathbf{R},\,\mathbf{t}} \; J(\mathbf{R}, \mathbf{t}) 
= \sum_{i=1}^{N} \left\| \mathbf{R}\,\mathbf{p}_i + \mathbf{t} - \mathbf{q}_i \right\|_2^2,
\quad \text{subject to } \mathbf{R}^\top \mathbf{R} = \mathbf{I},\; \det(\mathbf{R}) = +1.
\tag{1}
\end{equation}

The solution proceeds in three analytical steps:

\paragraph{Step 1: Centroid removal (translation decoupling)}  
Compute centroids of both point sets:
\begin{align}
\bar{\mathbf{p}} &= \frac{1}{N}\sum_{i=1}^{N} \mathbf{p}_i, &
\bar{\mathbf{q}} &= \frac{1}{N}\sum_{i=1}^{N} \mathbf{q}_i.
\end{align}
Define centered coordinates:
\begin{align}
\tilde{\mathbf{p}}_i &= \mathbf{p}_i - \bar{\mathbf{p}}, &
\tilde{\mathbf{q}}_i &= \mathbf{q}_i - \bar{\mathbf{q}}.
\end{align}
Substituting $\mathbf{t} = \bar{\mathbf{q}} - \mathbf{R}\bar{\mathbf{p}}$ into (1) eliminates the translation term, reducing the problem to pure rotation estimation:
\begin{equation}
\min_{\mathbf{R} \in \mathrm{SO}(3)} \; \sum_{i=1}^{N} \left\| \mathbf{R}\,\tilde{\mathbf{p}}_i - \tilde{\mathbf{q}}_i \right\|_2^2.
\tag{2}
\end{equation}

\paragraph{Step 2: Optimal rotation via SVD (Kabsch algorithm)}  
Construct the $3 \times 3$ cross-covariance matrix:
\begin{equation}
\mathbf{H} = \sum_{i=1}^{N} \tilde{\mathbf{q}}_i \, \tilde{\mathbf{p}}_i^\top 
= \mathbf{Q} \, \mathbf{P}^\top,
\tag{3}
\end{equation}
where $\mathbf{P} = [\tilde{\mathbf{p}}_1, \dots, \tilde{\mathbf{p}}_N] \in \mathbb{R}^{3\times N}$ and $\mathbf{Q} = [\tilde{\mathbf{q}}_1, \dots, \tilde{\mathbf{q}}_N] \in \mathbb{R}^{3\times N}$.

Perform singular value decomposition (SVD) of $\mathbf{H}$:
\begin{equation}
\mathbf{H} = \mathbf{U} \, \boldsymbol{\Sigma} \, \mathbf{V}^\top,
\tag{4}
\end{equation}
with $\mathbf{U}, \mathbf{V} \in \mathrm{SO}(3)$ (orthogonal matrices) and $\boldsymbol{\Sigma} = \mathrm{diag}(\sigma_1, \sigma_2, \sigma_3)$.

The optimal rotation minimizing (2) is then given by:
\begin{equation}
\mathbf{R}^\star = \mathbf{U} \, \mathbf{S} \, \mathbf{V}^\top,
\tag{5}
\end{equation}
where $\mathbf{S} = \mathrm{diag}(1, 1, \det(\mathbf{U}\mathbf{V}^\top))$ ensures $\det(\mathbf{R}^\star) = +1$ (enforcing proper rotation, not reflection).

\paragraph{Step 3: Translation recovery}  
With $\mathbf{R}^\star$ known, the optimal translation follows directly from centroid alignment:
\begin{equation}
\mathbf{t}^\star = \bar{\mathbf{q}} - \mathbf{R}^\star \bar{\mathbf{p}}.
\tag{6}
\end{equation}

\paragraph{Post-registration error metric}  
After applying $(\mathbf{R}^\star, \mathbf{t}^\star)$ to $\mathcal{P}$, the residual error for each correspondence is:
\begin{equation}
d_i = \left\| \mathbf{R}^\star \mathbf{p}_i + \mathbf{t}^\star - \mathbf{q}_i \right\|_2,
\quad i = 1,\dots,N,
\tag{7}
\end{equation}
and the root-mean-square error (RMSE) is computed as:
\begin{equation}
\mathrm{RMSE} = \sqrt{ \frac{1}{N} \sum_{i=1}^{N} d_i^2 }.
\tag{8}
\end{equation}

This closed-form solution is globally optimal under the rigid-body assumption and forms the mathematical backbone of the manual alignment routine implemented in CloudCompare’s \texttt{Align} plugin using user-selected landmark pairs.

\subsection{Point Cloud Similarity Metrics: F1-Score and Chamfer Distance}
\label{subsec:similarity_metrics}

To quantitatively evaluate the geometric fidelity between two facial point clouds—e.g., the reconstructed mesh from GS2Mesh (target) and the ground-truth scan from 3DMD (reference)—we adopt two complementary metrics: the \textbf{F1-score} (a symmetric precision-recall measure) and the \textbf{Chamfer distance} (an asymmetric but differentiable approximation of Hausdorff distance). Both are widely used in 3D shape comparison due to their robustness to sampling density, noise, and partial correspondence.

\paragraph{Chamfer Distance (CD)}  
Let $\mathcal{X} = \{\mathbf{x}_i\}_{i=1}^{N_X} \subset \mathbb{R}^3$ and $\mathcal{Y} = \{\mathbf{y}_j\}_{j=1}^{N_Y} \subset \mathbb{R}^3$ denote the two point sets (e.g., sampled vertices or downsampled points from the meshes). The (symmetric) Chamfer distance is defined as:

\begin{equation}
\mathrm{CD}(\mathcal{X}, \mathcal{Y}) 
= \frac{1}{N_X} \sum_{i=1}^{N_X} \min_{j} \|\mathbf{x}_i - \mathbf{y}_j\|_2^2 
\;+\; 
\frac{1}{N_Y} \sum_{j=1}^{N_Y} \min_{i} \|\mathbf{y}_j - \mathbf{x}_i\|_2^2.
\tag{9}
\end{equation}

In practice, the squared Euclidean norm is often used for numerical stability and differentiability (e.g., in deep learning pipelines); the unsquared version (using $\|\cdot\|_2$) is also common in evaluation benchmarks. CD penalizes both *missed structures* (points in $\mathcal{X}$ far from $\mathcal{Y}$) and *hallucinated structures* (points in $\mathcal{Y}$ far from $\mathcal{X}$), making it sensitive to global shape deviation while being computationally efficient ($\mathcal{O}(N_X N_Y)$, or $\mathcal{O}(N \log N)$ with k-d trees).

\paragraph{F1-Score}  
The F1-score is derived from precision and recall computed over nearest-neighbor correspondences at a fixed tolerance threshold $\tau > 0$. Define the set of true positives (TP) as:
\begin{equation}
\mathrm{TP} = \left\{ i \in [1,N_X] \;\middle|\; \min_j \|\mathbf{x}_i - \mathbf{y}_j\|_2 \leq \tau \right\},
\end{equation}
and similarly for reference points:
\begin{equation}
\mathrm{TP}' = \left\{ j \in [1,N_Y] \;\middle|\; \min_i \|\mathbf{y}_j - \mathbf{x}_i\|_2 \leq \tau \right\}.
\end{equation}
Then:
\begin{align}
\text{Precision} &= \frac{|\mathrm{TP}|}{N_X}, &
\text{Recall}    &= \frac{|\mathrm{TP}'|}{N_Y}.
\end{align}
The harmonic mean yields the F1-score:
\begin{equation}
\mathrm{F1}(\tau) = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}.
\tag{10}
\end{equation}

Unlike CD, F1-score is threshold-dependent and interpretable as a binary classification metric: it reflects the fraction of points within an acceptable error margin (e.g., $\tau = 1\,\text{mm}$ for facial geometry), thus aligning with perceptual or clinical relevance. A high F1-score indicates strong overlap in support—critical when evaluating whether fine facial features (e.g., nasal alae, lip contours) are preserved.

\paragraph{Why These Metrics?}  
- \textbf{Chamfer distance} provides a smooth, global scalar error suitable for optimization and ranking; it is less sensitive to outliers than Hausdorff distance and avoids the combinatorial complexity of exact correspondence.
- \textbf{F1-score} offers a human-interpretable performance gauge at clinically meaningful tolerances (e.g., sub-millimeter accuracy for surgical planning), and its symmetry ensures fairness when neither point set is strictly “ground truth” (e.g., in cross-method comparisons).
- Together, they mitigate each other’s weaknesses: CD may be dominated by dense regions, while F1-score ignores magnitude beyond $\tau$; using both gives a balanced view of *accuracy* (CD) and *completeness* (F1).

In our experiments, we report CD (in mm$^2$ or mm) and F1-score at $\tau = 0.5\,\text{mm}$, $1.0\,\text{mm}$, and $2.0\,\text{mm}$ to assess robustness across error scales.

\section{Experiments}
\subsection{Experimental Setup}

\textbf{Data Acquisition.} We conducted facial data collection in a clinical setting involving four healthy subjects (three females and one male, aged 20--30) with no craniofacial abnormalities. To establish high-fidelity ground truth, 3DMD photogrammetry was performed for each subject under the supervision of professional medical staff. Concurrently, video sequences were captured using a Realme Neo7 smartphone equipped with a Sony IMX882 sensor. The videos were recorded at 4K resolution ($3840 \times 2160$) and 60 fps. During acquisition, subjects maintained a neutral expression while allowing for natural micro-expressions. To ensure optimal geometric reconstruction and prevent artifacts, all subjects were captured without occlusions (e.g., glasses, jewelry, or makeup). The camera followed a circular trajectory at a distance of 0.5--1.0 m under controlled indoor lighting.

\textbf{Preprocessing and SfM.} Video frames were extracted via \texttt{ffmpeg} using uniform temporal sampling. The frame extraction frequency was treated as a primary independent variable for subsequent ablation studies. Structure-from-Motion (SfM) was implemented using the COLMAP GUI, where we systematically evaluated various feature extraction and matching algorithms. 

\textbf{3D Reconstruction and Alignment.} All reconstruction algorithms were executed on an NVIDIA GeForce RTX 4090 GPU (24GB VRAM). Given the scale ambiguity inherent in monocular SfM and the lack of pixel-level alignment between the 3DMD reference and the mobile-captured data, standard Iterative Closest Point (ICP) methods were prone to scale mismatch. Consequently, we employed a point-to-point manual alignment strategy in CloudCompare to calibrate the scale and orientation of the reconstructed point clouds against the 3DMD reference.

\bibliographystyle{unsrtnat}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\appendix

\section{Supplementary Details on Fisheye Camera Modeling}
\label{app:supp}

\subsection{Back-Projection via Newton–Raphson Iteration}
\label{app:backproj}

Given a pixel coordinate $(x, y)$ and intrinsic parameters $\boldsymbol{\theta}_{\mathrm{int}} = [f_x, f_y, c_x, c_y, k_1, k_2, k_3, k_4]^\top$, the back-projection to a unit ray $\mathbf{u} \in \mathbb{R}^3$ proceeds as follows:

\begin{algorithm}[H]
\caption{BackProject$(x, y, \boldsymbol{\theta}_{\mathrm{int}})$}
\label{alg:backproj}
\DontPrintSemicolon
$\tilde{x} \gets \dfrac{x - c_x}{f_x},\quad 
 \tilde{y} \gets \dfrac{y - c_y}{f_y}$\;
$r \gets \sqrt{\tilde{x}^2 + \tilde{y}^2},\quad 
\phi \gets \atan(\tilde{y}, \tilde{x})$\;
$\theta^{(0)} \gets r$ \tcp*[r]{initial guess}
\For{$n = 0$ \KwTo $N_{\text{iter}}-1$}{
    $f(\theta^{(n)}) \gets \theta^{(n)} + \sum_{\ell=1}^{4} k_\ell \left(\theta^{(n)}\right)^{2\ell+1}$\;
    $f'(\theta^{(n)}) \gets 1 + \sum_{\ell=1}^{4} (2\ell+1) k_\ell \left(\theta^{(n)}\right)^{2\ell}$\;
    $\theta^{(n+1)} \gets \theta^{(n)} - \dfrac{f(\theta^{(n)}) - r}{f'(\theta^{(n)})}$\;
    \lIf{$|\theta^{(n+1)} - \theta^{(n)}| < \epsilon_\theta$}{\textbf{break}}
}
$\mathbf{u} \gets 
\begin{bmatrix}
\sin\theta^{(n+1)} \cos\phi \\
\sin\theta^{(n+1)} \sin\phi \\
\cos\theta^{(n+1)}
\end{bmatrix}$\;
\Return $\mathbf{u}$\;
\end{algorithm}

\noindent
The iteration converges quadratically for $|\theta| < \pi/2$ and linearly near $\theta \to \pi$; in practice, $N_{\text{iter}} = 5$ suffices for sub-pixel accuracy ($\epsilon_\theta = 10^{-8}$ rad). This routine is used in BA (Alg.~\ref{alg:kb_ba}) to compute residuals and Jacobians.

\subsection{Analytic Jacobians for KB Reprojection Error}
\label{app:jacobian}

Let $\mathbf{r} = [r_x, r_y]^\top = \proj(\mathbf{X}_c; \boldsymbol{\theta}_{\mathrm{int}}) - \mathbf{x}$ be the reprojection residual. Denote $\mathbf{X}_c = [x_c, y_c, z_c]^\top$, $\rho = \|\mathbf{X}_c\|$, and $\mathbf{u} = \mathbf{X}_c / \rho$. Define:
\begin{align}
    \theta &= \arccos(u_z), &
    r &= f(\theta) = \theta + \sum_{\ell=1}^4 k_\ell \theta^{2\ell+1}, \\
    \phi &= \atan(u_y, u_x), &
    s &= \sin\theta,\; c = \cos\theta.
\end{align}
Then the partial derivatives are:

\paragraph{W.r.t. distortion coefficients $\mathbf{k} = [k_1, k_2, k_3, k_4]^\top$:}
\begin{equation}
    \frac{\partial \mathbf{r}}{\partial k_\ell}
    = 
    \begin{bmatrix}
        f_x \cdot \dfrac{\partial r}{\partial k_\ell} \cos\phi \\
        f_y \cdot \dfrac{\partial r}{\partial k_\ell} \sin\phi
    \end{bmatrix},
    \quad
    \frac{\partial r}{\partial k_\ell} = \theta^{2\ell+1}.
    \label{eq:dr_dk}
\end{equation}

\paragraph{W.r.t. rotation vector $\boldsymbol{\omega}$:}
Let $\mathbf{R} = \exp([\boldsymbol{\omega}]_\times)$ and $\delta\mathbf{R} = \frac{\partial \mathbf{R}}{\partial \omega_i} \mathbf{X}$. Then:
\begin{equation}
    \frac{\partial \mathbf{r}}{\partial \omega_i}
    = \frac{\partial \mathbf{r}}{\partial \mathbf{u}} 
      \frac{\partial \mathbf{u}}{\partial \mathbf{X}_c}
      \frac{\partial \mathbf{X}_c}{\partial \omega_i}
    = \mathbf{J}_{\mathrm{proj}} \cdot \mathbf{J}_{\mathrm{norm}} \cdot \left( [\mathbf{X}]_\times \mathbf{R}^\top \mathbf{u} \right),
    \label{eq:dr_domega}
\end{equation}
where
\begin{align}
    \mathbf{J}_{\mathrm{proj}} &= 
    \begin{bmatrix}
        f_x \cos\phi \cdot \dfrac{dr}{d\theta} & -f_x r \sin\phi \\
        f_y \sin\phi \cdot \dfrac{dr}{d\theta} &  f_y r \cos\phi
    \end{bmatrix}, \\
    \frac{dr}{d\theta} &= 1 + \sum_{\ell=1}^4 (2\ell+1) k_\ell \theta^{2\ell}, \\
    \mathbf{J}_{\mathrm{norm}} &= \frac{1}{\rho}
    \left( \mathbf{I} - \mathbf{u} \mathbf{u}^\top \right).
\end{align}

\paragraph{W.r.t. 3D point $\mathbf{X}$:}
\begin{equation}
    \frac{\partial \mathbf{r}}{\partial \mathbf{X}}
    = \mathbf{J}_{\mathrm{proj}} \, \mathbf{J}_{\mathrm{norm}} \, \mathbf{R}.
    \label{eq:dr_dX}
\end{equation}

These Jacobians enable exact first-order optimization in KB-BA without numerical differentiation. Implementation in PyTorch is straightforward using `torch.autograd.Function` with custom backward pass.

\subsection{Comparison with Pinhole Model}
\label{app:pinhole_vs_fisheye}

The standard pinhole projection assumes:
\begin{equation}
    r_{\text{pin}} = f \tan\theta, \quad \text{so} \quad 
    \frac{dr_{\text{pin}}}{d\theta} = f \sec^2\theta = f (1 + \tan^2\theta) = f \left(1 + \frac{r_{\text{pin}}^2}{f^2}\right).
    \label{eq:pinhole_dr}
\end{equation}
In contrast, the KB model uses a polynomial $r(\theta)$ with bounded derivative (since $|dr/d\theta| \leq 1 + \sum |(2\ell+1)k_\ell| \theta^{2\ell}$), avoiding the singularity at $\theta \to \pi/2$ inherent to pinhole models. This is critical for fisheye lenses where $\theta \in [0, \pi]$ (e.g., iPhone Ultra Wide: FoV $120^\circ \Rightarrow \theta_{\max} \approx 2.09$ rad).

Table~\ref{tab:model_comparison} summarizes key differences:

\begin{table}[h]
\centering
\caption{Pinhole vs. KB fisheye model properties.}
\label{tab:model_comparison}
\small
\begin{tabular}{lccc}
\toprule
Property & Pinhole & KB Fisheye & Advantage \\
\midrule
Projection law & $r = f \tan\theta$ & $r = \sum_{\ell=0}^4 a_\ell \theta^{2\ell+1}$ & Handles $\theta > \pi/2$ \\
Max FoV & $< 180^\circ$ (singularity) & $180^\circ$ (exact) & Full hemispherical coverage \\
Distortion params & None (ideal) & $k_1,\dots,k_4$ & Calibrates lens nonlinearity \\
Inverse projection & Closed-form: $\theta = \arctan(r/f)$ & Newton iteration (5 iters) & Slight overhead, but robust \\
BA Jacobian stability & Poor near $\theta \to \pi/2$ & Uniformly bounded & Better convergence \\
\bottomrule
\end{tabular}
\end{table}

This justifies our choice of KB model for smartphone fisheye reconstruction pipelines.

\subsection{Calibration Protocol for Smartphone Fisheye Cameras}
\label{app:calib_protocol}

We calibrate intrinsic parameters using a planar checkerboard captured from multiple poses (min. 15 images). Steps:
\begin{enumerate}
    \item Detect corners with subpixel accuracy (OpenCV \texttt{findChessboardCornersSB}).
    \item Initialize $f_x, f_y, c_x, c_y$ via Zhang’s method~\cite{zhang2000flexible} on undistorted (assumed pinhole) points.
    \item Refine all 8 intrinsics ($f_x,f_y,c_x,c_y,k_1..k_4$) by minimizing reprojection error under KB model (Eq.~\ref{eq:ba_loss} with fixed $\mathbf{X}_j$, $\mathbf{R}^{(i)},\mathbf{t}^{(i)}$).
    \item Use Levenberg–Marquardt with damping $\lambda=10^{-3}$; converge when $\Delta < 10^{-6}$ pixels.
\end{enumerate}
Typical calibrated values for iPhone 14 Ultra Wide ($\sim$13mm equiv.):
\[
f_x = f_y \approx 385.2,\;
c_x = 1080,\; c_y = 720,\;
\mathbf{k} = [-0.182,\, 0.041,\, -0.008,\, 0.001]^\top.
\]
These yield mean reprojection error $< 0.3$ px on validation images.

% Optional: Add citation key if needed
% \bibliographystyle{plain}
% \bibliography{reference}
\end{document}